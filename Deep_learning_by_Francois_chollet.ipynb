{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning by Francois chollet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Xog1aycrxF",
        "colab_type": "code",
        "outputId": "7107828c-3474-43da-b206-031146dbb665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip install tensorflow\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0rc1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0rc0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (46.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWdGmt0kpB7g",
        "colab_type": "text"
      },
      "source": [
        "**Chapter 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeETagAnpJ70",
        "colab_type": "text"
      },
      "source": [
        "**Problem 1** Imdb review classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijZGHJY0c3OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Binary classification with IMDB movie review dataset\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "(Xtrain,Ytrain),(Xtest,Ytest)= imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize(sequences,dimensions=10000):\n",
        "  tensor=np.zeros((len(sequences),dimensions))\n",
        "  for i,sequence in enumerate(sequences):\n",
        "    tensor[i,sequence]=1\n",
        "  return tensor\n",
        "tensor=vectorize(Xtrain)\n",
        "Xtrain=tensor\n",
        "Xtest=vectorize(Xtest)\n",
        "Ytrain=np.asarray(Ytrain).astype('float32')\n",
        "Ytest=np.asarray(Ytest).astype('float32')\n",
        "\n",
        "Xval=Xtrain[:10000]\n",
        "new_Xtrain=Xtrain[10000:]\n",
        "Yval=Ytrain[:10000]\n",
        "new_Ytrain=Ytrain[10000:]\n",
        "\n",
        "model=models.Sequential()\n",
        "model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))\n",
        "model.add(layers.Dense(16,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.fit(new_Xtrain,new_Ytrain,epochs=20,batch_size=512,validation_data=(Xval,Yval))\n",
        "model.evaluate(Xtest,Ytest,batch_size=512)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLM65-IF7aP2",
        "colab_type": "text"
      },
      "source": [
        "**Problem 2** \n",
        "News category prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA1BE9yniq0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import reuters\n",
        "import numpy as np\n",
        "from keras import models\n",
        "from keras import layers\n",
        "(Xtrain,Ytrain),(Xtest,Ytest)=reuters.load_data(num_words=10000)\n",
        "def vectorize(sequences,dimensions=10000):\n",
        "  tensor=np.zeros((len(sequences),dimensions))\n",
        "  for i,sequence in enumerate(sequences):\n",
        "    tensor[i,sequence]=1\n",
        "  return tensor\n",
        "Xtrain=vectorize(Xtrain)\n",
        "Xtest=vectorize(Xtest)\n",
        "Ytest= np.asarray(Ytest).astype('float32')\n",
        "Ytrain=np.asarray(Ytrain).astype('float32')\n",
        "Xval=Xtrain[:2000]\n",
        "Yval=Ytrain[:2000]\n",
        "Xtrain=Xtrain[2000:]\n",
        "Ytrain=Ytrain[2000:]\n",
        "\n",
        "model= models.Sequential()\n",
        "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
        "model.add(layers.Dense(64,activation='relu'))\n",
        "model.add(layers.Dense(46,activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(Xtrain,Ytrain,epochs=20,batch_size=512,validation_data=[Xval,Yval])\n",
        "model.evaluate(Xtest,Ytest)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXIYrBE7e35V",
        "colab_type": "text"
      },
      "source": [
        "**Problem 3**: Housing price prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNUUAbxUalLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "(Xtrain,Ytrain),(Xtest,Ytest)=boston_housing.load_data()\n",
        "Xmean=Xtrain.mean(axis=0)\n",
        "Xtrain-=Xmean\n",
        "Xtest-=Xmean\n",
        "Xstd=Xtrain.std(axis=0)\n",
        "Xtrain/=Xstd\n",
        "Xtest/=Xstd\n",
        "\n",
        "def build_model():\n",
        "  model=models.Sequential()\n",
        "  model.add(layers.Dense(64,activation='relu',input_shape=(Xtrain.shape[1],)))\n",
        "  model.add(layers.Dense(64,activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n",
        "  return model\n",
        "\n",
        "k=4\n",
        "num_segments=len(Xtrain)//k\n",
        "model_outputs=[]\n",
        "\n",
        "for i in range(k):\n",
        "  val_x=Xtrain[num_segments*i:num_segments*(i+1)]\n",
        "  val_y=Ytrain[num_segments*i:num_segments*(i+1)]\n",
        "  Xtrain_set= np.concatenate([Xtrain[:num_segments*i],Xtrain[num_segments*(i+1):]],axis=0)\n",
        "  Ytrain_set=np.concatenate([Ytrain[:num_segments*i],Ytrain[num_segments*(i+1):]],axis=0)\n",
        "  model=build_model()\n",
        "  model.fit(Xtrain_set,Ytrain_set,epochs=100,batch_size=1)\n",
        "  mse,mae=model.evaluate(val_x,val_y)\n",
        "  model_outputs.append(mae)\n",
        "model_mean=np.mean(model_outputs)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Pov5KQLZa3",
        "colab_type": "text"
      },
      "source": [
        "**Chapter 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn_YGZ2MI_7R",
        "colab_type": "text"
      },
      "source": [
        "**Problem 1 (Cats and Dogs classification)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eoav_7SpPxPr",
        "colab_type": "text"
      },
      "source": [
        "Training a network from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlMu-FrNLcFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setting up the kaggle api to download datasets directly from kaggle\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets list\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "\n",
        "#arranging the downloaded dataset into train, test, and validation file folders\n",
        "!unzip train.zip \n",
        "!unzip test1.zip\n",
        "!rm train.zip\n",
        "!rm test1.zip\n",
        "!mkdir train_set\n",
        "!mkdir test_set\n",
        "%cd train_set\n",
        "!mkdir train_dog\n",
        "!mkdir train_cat\n",
        "%cd ..\n",
        "%cd test_set\n",
        "!mkdir test_dog\n",
        "!mkdir test_cat\n",
        "%cd ..\n",
        "!mkdir val_set\n",
        "%cd val_set\n",
        "!mkdir val_cat\n",
        "!mkdir val_dog\n",
        "%cd ..\n",
        "import shutil\n",
        "fnames_cat =['cat.{}.jpg'.format(i) for i in range(1000) ]\n",
        "for i in fnames_cat:\n",
        "  shutil.copy('train/'+i,'train_set/train_cat')\n",
        "fnames_dog=['dog.{}.jpg'.format(i) for i in range(1000)]\n",
        "for i in fnames_dog:\n",
        "  shutil.copy('train/'+i,'train_set/train_dog')\n",
        "fnames_cat =['cat.{}.jpg'.format(i) for i in range(1001,1501) ]\n",
        "for i in fnames_cat:\n",
        "  shutil.copy('train/'+i,'test_set/test_cat')\n",
        "fnames_dog =['dog.{}.jpg'.format(i) for i in range(1001,1501) ]\n",
        "for i in fnames_dog:\n",
        "  shutil.copy('train/'+i,'test_set/test_dog')\n",
        "fnames_cat =['cat.{}.jpg'.format(i) for i in range(1502,2002) ]\n",
        "for i in fnames_cat:\n",
        "  shutil.copy('train/'+i,'val_set/val_cat')\n",
        "fnames_dog =['dog.{}.jpg'.format(i) for i in range(1502,2002) ]\n",
        "for i in fnames_dog:\n",
        "  shutil.copy('train/'+i,'val_set/val_dog')\n",
        "\n",
        "#Preprocessing the image and convert it into a tensor format so that the neural network can accept it as an input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen=ImageDataGenerator(rescale=1./255)\n",
        "traingen=datagen.flow_from_directory('train_set',target_size=(150,150),batch_size=20,class_mode='binary')\n",
        "testgen=datagen.flow_from_directory('test_set',target_size=(150,150),batch_size=20,class_mode='binary')\n",
        "valgen=datagen.flow_from_directory('val_set',target_size=(150,150),batch_size=20,class_mode='binary') \n",
        "\n",
        "\n",
        "#building the model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "def build_model():\n",
        "  model=models.Sequential()\n",
        "  model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(512,activation='relu'))\n",
        "  model.add(layers.Dense(1,activation='sigmoid'))\n",
        "  model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "#training the model, saving and downloading the model file\n",
        "model=build_model()\n",
        "model.fit_generator(traingen,epochs=30,validation_data=valgen,steps_per_epoch=100,validation_steps=50)\n",
        "model.save('cats_vs_dog.h5')  \n",
        "files.download('cats_vs_dog.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxGyt4JBP2yc",
        "colab_type": "text"
      },
      "source": [
        "Using a pretrained network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqI0XlfliDMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras import models,layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "conv_base=VGG19(weights='imagenet',include_top=False,input_shape=(150,150,3))\n",
        "conv_base.trainable=False\n",
        "model=models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "datagen=ImageDataGenerator(rescale=1./255,rotation_range=40,height_shift_range=0.2,width_shift_range=0.2,shear_range=0.2,horizontal_flip=True,fill_mode='nearest')\n",
        "traingen= datagen.flow_from_directory('train_set',target_size=(150,150),batch_size=20,class_mode='binary')\n",
        "valgen= datagen.flow_from_directory('val_set',target_size=(150,150),batch_size=20,class_mode='binary')\n",
        "model.fit_generator(traingen,steps_per_epoch=100,epochs=30,validation_data=valgen,validation_steps=20)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ7WCOEwWQrz",
        "colab_type": "text"
      },
      "source": [
        "Chapter 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPyk-iOQWT-u",
        "colab_type": "text"
      },
      "source": [
        "**Word level one hot encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akUFOXXBDD1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "samples=['the cat sat on a mat','He took the dog for a walk']\n",
        "\n",
        "sample_dict={}\n",
        "for sample in samples:\n",
        "  for word in sample.split(\" \"):\n",
        "    if word not in sample_dict.keys():\n",
        "      sample_dict[word]=len(sample_dict)+1\n",
        "\n",
        "max_len=10\n",
        "\n",
        "results=np.zeros((len(samples),max_len,max(sample_dict.values())+1))\n",
        "for i,sample in enumerate(samples):\n",
        "  for j,words in list(enumerate(sample.split(\" \")))[:max_len]:\n",
        "    index=sample_dict.get(words)\n",
        "    print(words,index)\n",
        "    results[i,j,index]=1\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSduZRyshcE",
        "colab_type": "text"
      },
      "source": [
        "**Character level One Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id01AGwCrWIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "samples=['the cat sat on a mat','He took the dog for a walk']\n",
        "\n",
        "characters= string.printable\n",
        "token_index=dict(zip(range(1,len(characters)+1),characters))\n",
        "max_len=50\n",
        "\n",
        "results=np.zeros((len(samples),max_len,len(token_index.keys())+1))\n",
        "\n",
        "for i,sample in enumerate(samples):\n",
        "  for j,character in enumerate(sample):\n",
        "    index=token_index.get(character)\n",
        "    print(index)\n",
        "    results[i,j,index]=1\n",
        "print(token_index,results)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap6io-oXtcIX",
        "colab_type": "text"
      },
      "source": [
        "**Creating word embeddings using layers.Embedding()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fj_2I5RszEt",
        "colab_type": "code",
        "outputId": "69b405be-5f2c-4031-e79d-d10d53b95e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "from tensorflow.keras import layers,models,preprocessing\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "maxlen=20\n",
        "(Xtrain,Ytrain),(Xtest,Ytest)=imdb.load_data(num_words=1000)\n",
        "Xtrain= preprocessing.sequence.pad_sequences(Xtrain,maxlen=maxlen)\n",
        "Xtest=preprocessing.sequence.pad_sequences(Xtest,maxlen=maxlen)\n",
        "model= models.Sequential()\n",
        "model.add(layers.Embedding(1000,8,input_length=20))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(Xtrain,Ytrain,epochs=20)\n",
        "model.evaluate(Xtest,Ytest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.6207 - accuracy: 0.6470\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.5157 - accuracy: 0.7434\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.4962 - accuracy: 0.7590\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4833 - accuracy: 0.7678\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4704 - accuracy: 0.7780\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4588 - accuracy: 0.7833\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4494 - accuracy: 0.7916\n",
            "Epoch 8/20\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.4408 - accuracy: 0.7956\n",
            "Epoch 9/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4340 - accuracy: 0.8016\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.4278 - accuracy: 0.8045\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4218 - accuracy: 0.8075\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4184 - accuracy: 0.8121\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.4133 - accuracy: 0.8148\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.4094 - accuracy: 0.8174\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4056 - accuracy: 0.8166\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4020 - accuracy: 0.8194\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3988 - accuracy: 0.8247\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3961 - accuracy: 0.8244\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3938 - accuracy: 0.8279\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.3906 - accuracy: 0.8292\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6211 - accuracy: 0.7055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6211338639259338, 0.7054799795150757]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUwCVNTRP7Ke",
        "colab_type": "text"
      },
      "source": [
        "**Classification using pretrained embeddings (GLoVE)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j00deIg4u3jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download the raw text dataset\n",
        "!wget --no-check-certificate \"https://mng.bz/0tIo\"\n",
        "!unzip \"0tIo\"\n",
        "\n",
        "#split the dataset into corresponding train and test files\n",
        "%cd aclImdb\n",
        "from os import listdir\n",
        "train_text=[]\n",
        "train_labels=[]\n",
        "test_text=[]\n",
        "test_labels=[]\n",
        "\n",
        "for file_type in ['neg','pos']:\n",
        "  for file in listdir('train/'+file_type):\n",
        "    f=open('train/'+file_type+'/'+file)\n",
        "    train_text.append(f.read())\n",
        "    if file_type=='neg':\n",
        "      train_labels.append(0)\n",
        "    else:\n",
        "      train_labels.append(1)\n",
        "\n",
        "\n",
        "for file_type in ['neg','pos']:\n",
        "  for file in listdir('test/'+file_type):\n",
        "    f=open('test/'+file_type+'/'+file)\n",
        "    test_text.append(f.read())\n",
        "    if file_type=='neg':\n",
        "      test_labels.append(0)\n",
        "    else:\n",
        "      test_labels.append(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_CmVFO9L4QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocessing the raw text\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen=100\n",
        "maxsize=10000\n",
        "train_size=200\n",
        "val_size=10000\n",
        "\n",
        "#breaking down sentences into tokens and converting them into vectores \n",
        "token=Tokenizer(num_words=maxsize)\n",
        "token.fit_on_texts(train_text)\n",
        "sequence=token.texts_to_sequences(train_text)\n",
        "data=pad_sequences(sequence,maxlen)\n",
        "word_index=token.word_index\n",
        "train_labels=np.asarray(train_labels)\n",
        "\n",
        "#shuffling the dataset, as it is present in a sequence (neg -> pos)\n",
        "indices=np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "train_text=data[indices]\n",
        "train_labels=train_labels[indices]\n",
        "\n",
        "#generating the training and validation set\n",
        "val_text=train_text[train_size+1:val_size]\n",
        "val_labels=train_labels[train_size+1:val_size]\n",
        "train_text=train_text[:train_size]\n",
        "train_labels=train_labels[:train_size]\n",
        "%cd ..\n",
        "\n",
        "#Downloading stanfords glove pretrained embedding model\n",
        "!wget 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "!unzip 'glove.6B.zip'\n",
        "\n",
        "#creating a dictinary with words and their corresponding vectors from the Glove model\n",
        "from os import listdir\n",
        "f=open('glove.6B.100d.txt')\n",
        "embed={}\n",
        "for line in f:\n",
        "  sentence=line.split(\" \")\n",
        "  word=sentence[0]\n",
        "  value=np.asarray(sentence[1:]).astype('float32')\n",
        "  embed[word]=value\n",
        "\n",
        "\n",
        "#Building the embedding matrix which will be input into the embedding layer\n",
        "\n",
        "embedding_dim=100\n",
        "\n",
        "embed_matrix=np.zeros((maxsize,embedding_dim))\n",
        "for word,i in word_index.items():\n",
        "  if i<maxsize:\n",
        "    embed_vector=embed.get(word)\n",
        "    if embed_vector is not None:\n",
        "      embed_matrix[i]=embed_vector\n",
        "\n",
        "#Building, training and testing the model\n",
        "from tensorflow.keras import layers,models\n",
        "\n",
        "model=models.Sequential()\n",
        "model.add(layers.Embedding(maxsize,embedding_dim,input_length=maxlen))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid')) \n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.layers[0].set_weights([embed_matrix])\n",
        "model.layers[0].trainable=False\n",
        "model.fit(train_text,train_labels,epochs=20,validation_data=(val_text,val_labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuK93xHGypdS",
        "colab_type": "text"
      },
      "source": [
        "**Temperature forecasting problem**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni9ftm_eihPp",
        "colab_type": "code",
        "outputId": "9935ff5b-3598-4867-eb8d-b9d04a04a4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Download the data and unzip the files\n",
        "!wget 'https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip'\n",
        "!unzip jena_climate_2009_2016.csv.zip\n",
        "rm jena_climate_2009_2016.csv.zip\n",
        "\n",
        "#read the data from csv into numpy arrays\n",
        "f=open('jena_climate_2009_2016.csv')\n",
        "f=f.read()\n",
        "lines=f.split('\\n')\n",
        "header=lines[0].split(',')\n",
        "lines=lines[1:]\n",
        "import numpy as np\n",
        "float_data=np.zeros((len(lines),len(header)-1))\n",
        "for i,line in enumerate(lines):\n",
        "  values=[float(x) for x in line.split(\",\")[1:]]\n",
        "  float_data[i,:]=values\n",
        "\n",
        "\n",
        "#Normalizing the data using standard normalization\n",
        "data_mean=float_data[:200000].mean(axis=0)\n",
        "float_data-=data_mean\n",
        "data_std=float_data[:200000].std(axis=0)\n",
        "float_data/=data_std\n",
        "\n",
        "\n",
        "#Creating a python generator function to format the data in a way, the neural nets can accept them\n",
        "def datagen(data,lookback,delay,shuffle,min_index,max_index,batch_size,step):\n",
        "  if max_index is None:\n",
        "    max_index=len(data)-delay-1\n",
        "  i=min_index+lookback\n",
        "  while 1:\n",
        "    if shuffle:\n",
        "      row=np.random.randint(i,max_index,size=batch_size)\n",
        "    else:\n",
        "\n",
        "      if i+batch_size<=max_index:\n",
        "        i=min_index+lookback\n",
        "      row=np.arange(i,min(i+batch_size,max_index))\n",
        "      i+=len(row)\n",
        "      samples=np.zeros((len(row),lookback//step,data.shape[-1]))\n",
        "      target=np.zeros((len(row),))\n",
        "\n",
        "      for j,r in enumerate(row):\n",
        "        indices=range(row[j]-lookback,row[j],step)\n",
        "        samples[j]=data[indices]\n",
        "        target[j]=data[row[j]+delay][1]\n",
        "      yield samples,target\n",
        "\n",
        "#Getting the train, test, and validation sets    \n",
        "lookback=1440\n",
        "step=6\n",
        "batch_size=128\n",
        "delay=144\n",
        "train_gen=datagen(float_data,lookback,delay,shuffle=False,min_index=0,max_index=200000,batch_size=128,step=6)\n",
        "test_gen=datagen(float_data,lookback,delay,shuffle=False,min_index=200000,max_index=300000,batch_size=128,step=6)\n",
        "val_gen=datagen(float_data,lookback,delay,shuffle=False,min_index=300000,max_index=None,batch_size=128,step=6)\n",
        "\n",
        "\n",
        "#Building our models: Stacked GRU with dropouts, Bidirectional RNN's\n",
        "from tensorflow.keras import layers,models\n",
        "\n",
        "model=models.Sequential()\n",
        "#Using stacked gru's with recurrent dropout\n",
        "# model.add(layers.GRU(32,dropout=0.1,recurrent_dropout=0.5,return_sequences=True,input_shape=(None,float_data.shape[-1])))\n",
        "# model.add(layers.GRU(64,dropout=0.1,recurrent_dropout=0.5,activation='relu'))\n",
        "\n",
        "#Using bidirectional rnn\n",
        "#model.add(layers.Bidirectional(layers.GRU(32),input_shape=(None,float_data.shape[-1])))\n",
        "\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer='rmsprop',loss='mae')\n",
        "model.fit(train_gen,epochs=40,steps_per_epoch=500,validation_data=val_gen,validation_steps=200)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-31 18:12:06--  https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.237.157\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.237.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13568290 (13M) [application/zip]\n",
            "Saving to: ‘jena_climate_2009_2016.csv.zip’\n",
            "\n",
            "jena_climate_2009_2 100%[===================>]  12.94M  18.3MB/s    in 0.7s    \n",
            "\n",
            "2020-03-31 18:12:07 (18.3 MB/s) - ‘jena_climate_2009_2016.csv.zip’ saved [13568290/13568290]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKiFGRsafnSh",
        "colab_type": "text"
      },
      "source": [
        "**Chapter 8: Generative Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHuzZawpzj6L",
        "colab_type": "text"
      },
      "source": [
        "**Text generation with LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsvSspgbEswz",
        "colab_type": "code",
        "outputId": "6d6ad981-5d52-4eb7-f665-2c5e752b313a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow.keras as tfk\n",
        "import numpy as np\n",
        "\n",
        "path=tfk.utils.get_file('nietzsche.txt',origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "text=open(path).read().lower()\n",
        "print(len(text))\n",
        "maxlen=60\n",
        "step=3\n",
        "sentences=[]\n",
        "next_chars=[]\n",
        "#for i in range(0,len(text)-maxlen,step):"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UljNpsgjzm6m",
        "colab_type": "text"
      },
      "source": [
        "**Deep Dream**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxEoZuyVYG8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications import inception_v3\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_training_phase(0)\n",
        "model=inception_v3.InceptionV3(weights='imagenet',include_top=False)\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}